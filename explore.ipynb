{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32390453",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f489ae42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "data_dir = Path(\"data\")\n",
    "imgs_dir = data_dir / \"things-eeg2-imgs\"\n",
    "eeg_dir = data_dir / \"things-eeg2-pre\"\n",
    "\n",
    "\n",
    "ex_data_path = \"data/things-eeg2-pre/sub-01/preprocessed_eeg_training.npy\"\n",
    "ex_data = np.load(ex_data_path, allow_pickle=True).item()\n",
    "ex_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d122658",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_paths = [\n",
    "    img_dir\n",
    "    for concept_dir in sorted((imgs_dir / \"training_images\").iterdir())\n",
    "    for img_dir in sorted(concept_dir.iterdir())\n",
    "]\n",
    "test_img_paths = [\n",
    "    img_dir\n",
    "    for concept_dir in sorted((imgs_dir / \"test_images\").iterdir())\n",
    "    for img_dir in sorted(concept_dir.iterdir())\n",
    "]\n",
    "\n",
    "train_imgs_per_concept = 10\n",
    "test_imgs_per_concept = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3270662",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "\n",
    "def idx_to_img_path(idx: int, split: Literal[\"train\", \"test\"]) -> Path:\n",
    "    if split == \"train\":\n",
    "        paths = train_img_paths\n",
    "\n",
    "    elif split == \"test\":\n",
    "        paths = test_img_paths\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid split: {split}\")\n",
    "\n",
    "    return paths[idx]\n",
    "\n",
    "\n",
    "print(\"0\", idx_to_img_path(0, \"train\"))  # Aardvark 1\n",
    "print(\"10\", idx_to_img_path(10, \"train\"))  # Acabus 1\n",
    "print(\"11\", idx_to_img_path(11, \"train\"))  # Acabus 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b650a2",
   "metadata": {},
   "source": [
    "## Image Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41b2e53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.transforms import v2 as tv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def load_img(idx: int, split: Literal[\"train\", \"test\"]):\n",
    "    path = idx_to_img_path(idx, split)\n",
    "    img = torchvision.io.decode_image(str(path))\n",
    "    return img\n",
    "\n",
    "\n",
    "img = load_img(0, \"train\")\n",
    "\n",
    "print(\"Size:\", img.size())\n",
    "print(\"Dtype:\", img.dtype)\n",
    "print(\"Min\", \"Max:\", img.min(), img.max())\n",
    "\n",
    "tv2.functional.to_pil_image(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c97ad24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def preprocess_img(img: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Preprocess image for by resizing to 256, then normalizing to [0,1].\"\"\"\n",
    "\n",
    "    img = tv2.functional.resize(img, [256])\n",
    "    img_pp = img / 255.0\n",
    "    return img_pp\n",
    "\n",
    "\n",
    "img_pp = preprocess_img(img)\n",
    "print(\"Size:\", img_pp.size())\n",
    "print(\"Dtype:\", img_pp.dtype)\n",
    "print(\"Min\", \"Max:\", img_pp.min(), img_pp.max())\n",
    "\n",
    "plt.imshow(img_pp.permute(1, 2, 0).numpy())\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5060aacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "img2_pp = load_img(11, \"train\")\n",
    "img2_pp = preprocess_img(img2_pp)\n",
    "plt.imshow(img2_pp.permute(1, 2, 0).numpy())\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795f16ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_img_pp_05 = torch.clamp(img_pp + 0.5 * torch.randn_like(img_pp), 0, 1)\n",
    "noisy_img_pp_005 = torch.clamp(img_pp + 0.05 * torch.randn_like(img_pp), 0, 1)\n",
    "plt.imshow(noisy_img_pp_05.permute(1, 2, 0).numpy())\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8938dc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dreamsim.model import dreamsim\n",
    "\n",
    "\n",
    "dreamsim_model, _ = dreamsim(dreamsim_type=\"synclr_vitb16\")\n",
    "dreamsim_model.requires_grad_(False)\n",
    "dreamsim_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8bc0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_img_to_img2 = dreamsim_model(\n",
    "    img_pp.unsqueeze(0).to(dreamsim_model.device),\n",
    "    img2_pp.unsqueeze(0).to(dreamsim_model.device),\n",
    ")\n",
    "sim_img_to_noise_005 = dreamsim_model(\n",
    "    img_pp.unsqueeze(0).to(dreamsim_model.device),\n",
    "    noisy_img_pp_005.unsqueeze(0).to(dreamsim_model.device),\n",
    ")\n",
    "sim_img_to_noise_05 = dreamsim_model(\n",
    "    img_pp.unsqueeze(0).to(dreamsim_model.device),\n",
    "    noisy_img_pp_05.unsqueeze(0).to(dreamsim_model.device),\n",
    ")\n",
    "\n",
    "print(\"Dreamsim:\")\n",
    "print(\"\\tSimilarity img to img2:\", sim_img_to_img2.item())\n",
    "print(\"\\tSimilarity img to noise (0.05):\", sim_img_to_noise_005.item())\n",
    "print(\"\\tSimilarity img to noise (0.5):\", sim_img_to_noise_05.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264ea70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_synclr_model = dreamsim_model.base_model.model.extractor_list[0].model\n",
    "aligned_synclr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885f4de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dreamsim.feature_extraction.load_synclr_as_dino import load_synclr_as_dino\n",
    "\n",
    "synclr_model = load_synclr_as_dino(16)\n",
    "synclr_model.requires_grad_(False)\n",
    "synclr_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899f70d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosine_sim(x1: torch.Tensor, x2: torch.Tensor) -> torch.Tensor:\n",
    "    x1 = x1.flatten(start_dim=1)\n",
    "    x2 = x2.flatten(start_dim=1)\n",
    "    x1 = x1 / torch.norm(x1, dim=-1, keepdim=True)\n",
    "    x2 = x2 / torch.norm(x2, dim=-1, keepdim=True)\n",
    "    return (x1 @ x2.T).squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b5b545",
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 = synclr_model(img_pp.unsqueeze(0))\n",
    "z2 = synclr_model(img2_pp.unsqueeze(0))\n",
    "zn005 = synclr_model(noisy_img_pp_005.unsqueeze(0))\n",
    "zn05 = synclr_model(noisy_img_pp_05.unsqueeze(0))\n",
    "\n",
    "sim_img_to_img2 = get_cosine_sim(z1, z2)\n",
    "sim_img_to_noise_005 = get_cosine_sim(z1, zn005)\n",
    "sim_img_to_noise_05 = get_cosine_sim(z1, zn05)\n",
    "\n",
    "print(\"SimClr:\")\n",
    "print(\"\\tSimilarity img to img2:\", sim_img_to_img2.item())\n",
    "print(\"\\tSimilarity img to noise (0.05):\", sim_img_to_noise_005.item())\n",
    "print(\"\\tSimilarity img to noise (0.5):\", sim_img_to_noise_05.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25659dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "z1 = aligned_synclr_model(img_pp.unsqueeze(0).to(device))\n",
    "z2 = aligned_synclr_model(img2_pp.unsqueeze(0).to(device))\n",
    "zn005 = aligned_synclr_model(noisy_img_pp_005.unsqueeze(0).to(device))\n",
    "zn05 = aligned_synclr_model(noisy_img_pp_05.unsqueeze(0).to(device))\n",
    "\n",
    "sim_img_to_img2 = get_cosine_sim(z1, z2)\n",
    "sim_img_to_noise_005 = get_cosine_sim(z1, zn005)\n",
    "sim_img_to_noise_05 = get_cosine_sim(z1, zn05)\n",
    "\n",
    "print(\"Aligned SimClr:\")\n",
    "print(\"\\tSimilarity img to img2:\", sim_img_to_img2.item())\n",
    "print(\"\\tSimilarity img to noise (0.05):\", sim_img_to_noise_005.item())\n",
    "print(\"\\tSimilarity img to noise (0.5):\", sim_img_to_noise_05.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8427219e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "GEN_EMBEDDINGS = False\n",
    "\n",
    "\n",
    "def gen_embeddings(\n",
    "    model: torch.nn.Module,\n",
    "    img_paths: list[Path],\n",
    "    split: Literal[\"train\", \"test\"],\n",
    "    batch_size: int = 32,\n",
    "    device: torch.device = device,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Generate embeddings for a given split of images.\"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm.tqdm(range(0, len(img_paths), batch_size)):\n",
    "            imgs = [\n",
    "                load_img(j, split=split)\n",
    "                for j in range(i, min(i + batch_size, len(img_paths)))\n",
    "            ]\n",
    "            imgs = [preprocess_img(img).unsqueeze(0) for img in imgs]\n",
    "            imgs = torch.cat(imgs, dim=0).to(device)\n",
    "            emb = model(imgs)\n",
    "            embeddings.append(emb.cpu().numpy())\n",
    "    return np.concatenate(embeddings, axis=0)\n",
    "\n",
    "\n",
    "embed_dir = data_dir / \"things-eeg2-imgs-embed\"\n",
    "for model, name in [\n",
    "    (synclr_model, \"synclr\"),\n",
    "    (aligned_synclr_model, \"aligned_synclr\"),\n",
    "]:\n",
    "    if GEN_EMBEDDINGS:\n",
    "        print(f\"Generating {name} embeddings...\")\n",
    "\n",
    "        train_embeddings = gen_embeddings(\n",
    "            model, train_img_paths, split=\"train\", batch_size=512\n",
    "        )\n",
    "        np.save(f\"{embed_dir / name}/train_embeddings.npy\", train_embeddings)\n",
    "\n",
    "        test_embeddings = gen_embeddings(\n",
    "            model, test_img_paths, split=\"test\", batch_size=512\n",
    "        )\n",
    "        np.save(f\"{embed_dir / name}/test_embeddings.npy\", test_embeddings)\n",
    "\n",
    "    else:\n",
    "        print(\"Loading embeddings...\")\n",
    "\n",
    "        train_embeddings = np.load(f\"{embed_dir / name}/train_embeddings.npy\")\n",
    "        test_embeddings = np.load(f\"{embed_dir / name}/test_embeddings.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2294e65c",
   "metadata": {},
   "source": [
    "## EEG Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e0de24",
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_names = ex_data[\"ch_names\"]\n",
    "print(f\"Channel names: {', '.join(channel_names)}\")\n",
    "\n",
    "eeg_data = ex_data[\"preprocessed_eeg_data\"]\n",
    "print(\"EEG data: \")\n",
    "print(f\"\\tShape: {eeg_data.shape}\")\n",
    "print(f\"\\tData type: {eeg_data.dtype}\")\n",
    "print(f\"\\tMaximum value: {np.max(eeg_data)}\")\n",
    "print(f\"\\tMinimum value: {np.min(eeg_data)}\")\n",
    "print(f\"\\tMean value: {np.mean(eeg_data)}\")\n",
    "print(f\"\\tStandard deviation: {np.std(eeg_data)}\")\n",
    "\n",
    "times = ex_data[\"times\"]\n",
    "print(f\"Times: {times}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2214d38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_eeg_data(eeg_data):\n",
    "    \"\"\"Preprocess the EEG data by averaging over the number of repetitions.\n",
    "\n",
    "    Args:\n",
    "        eeg_data (numpy.ndarray): The EEG data to preprocess. <concepts, repetitions, channels, timesteps>\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The preprocessed EEG data. <concepts, channels, timesteps>\n",
    "    \"\"\"\n",
    "    # Average over the number of repetitions\n",
    "    preprocessed_data = np.mean(eeg_data, axis=1).astype(np.float32)\n",
    "    return preprocessed_data\n",
    "\n",
    "\n",
    "eeg_data_pp = preprocess_eeg_data(eeg_data)\n",
    "eeg_data_pp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367096f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_eeg_data(eeg_path: Path):\n",
    "    \"\"\"Load EEG data from a given path.\"\"\"\n",
    "    eeg_data = np.load(eeg_path, allow_pickle=True).item()\n",
    "    return (\n",
    "        preprocess_eeg_data(eeg_data[\"preprocessed_eeg_data\"]),\n",
    "        eeg_data[\"times\"],\n",
    "        eeg_data[\"ch_names\"],\n",
    "    )\n",
    "\n",
    "\n",
    "def load_all_eeg_data(\n",
    "    eeg_paths: list[Path],\n",
    ") -> tuple[torch.Tensor, torch.Tensor, list[str]]:\n",
    "    all_eeg_data = []\n",
    "    all_times = None\n",
    "    all_ch_names = []\n",
    "\n",
    "    for eeg_path in eeg_paths:\n",
    "        eeg_data, times, ch_names = load_eeg_data(eeg_path)\n",
    "        all_eeg_data.append(torch.from_numpy(eeg_data))\n",
    "\n",
    "        if all_times is None:\n",
    "            all_times = torch.tensor(times)\n",
    "\n",
    "        if not all_ch_names:\n",
    "            all_ch_names = ch_names\n",
    "\n",
    "    if all_times is None:\n",
    "        all_times = torch.tensor([])\n",
    "\n",
    "    return torch.concat(all_eeg_data), all_times, all_ch_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee55e909",
   "metadata": {},
   "source": [
    "## Unified Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bdc597",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class EEGDatasetConfig(BaseModel):\n",
    "    data_path: Path = Path(\"data\")\n",
    "    imgs_dir: str = \"things-eeg2-imgs\"\n",
    "    eeg_dir: str = \"things-eeg2-pre\"\n",
    "    imgs_latent_dir: str = \"things-eeg2-imgs-embed\"\n",
    "    train_imgs_per_concept: int = 10\n",
    "    test_imgs_per_concept: int = 1\n",
    "    subs: list[int] = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: EEGDatasetConfig,\n",
    "        split: Literal[\"train\", \"test\"],\n",
    "        model_name: str = \"synclr\",\n",
    "    ):\n",
    "        self.config = config\n",
    "        self.split = split\n",
    "        self.model_name = model_name\n",
    "        self.imgs_per_concepts = (\n",
    "            self.config.train_imgs_per_concept\n",
    "            if split == \"train\"\n",
    "            else self.config.test_imgs_per_concept\n",
    "        )\n",
    "\n",
    "        if split == \"train\":\n",
    "            img_subdir = \"training_images\"\n",
    "            img_embed_name = \"train_embeddings\"\n",
    "            eeg_name = \"preprocessed_eeg_training\"\n",
    "\n",
    "        else:\n",
    "            img_subdir = \"test_images\"\n",
    "            img_embed_name = \"test_embeddings\"\n",
    "            eeg_name = \"preprocessed_eeg_test\"\n",
    "\n",
    "        self.img_paths = [\n",
    "            img_dir\n",
    "            for concept_dir in sorted(\n",
    "                (self.config.data_path / self.config.imgs_dir / img_subdir).iterdir()\n",
    "            )\n",
    "            for img_dir in sorted(concept_dir.iterdir())\n",
    "        ]\n",
    "        self.img_latents = torch.from_numpy(\n",
    "            np.load(\n",
    "                self.config.data_path\n",
    "                / f\"{self.config.imgs_latent_dir}/{model_name}/{img_embed_name}.npy\"\n",
    "            )\n",
    "        ).float()\n",
    "\n",
    "        self.eeg_data_paths = [\n",
    "            self.config.data_path\n",
    "            / self.config.eeg_dir\n",
    "            / f\"sub-{sub:02}\"\n",
    "            / f\"{eeg_name}.npy\"\n",
    "            for sub in self.config.subs\n",
    "        ]\n",
    "        self.eeg_data, self.times, self.ch_names = load_all_eeg_data(\n",
    "            self.eeg_data_paths\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.eeg_data)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        img_idx = idx % (\n",
    "            len(self.img_paths)\n",
    "        )  # EEG has stacked over subs, so we need to find the right sample within the sub\n",
    "\n",
    "        img_path = self.img_paths[img_idx]\n",
    "        img_latent = self.img_latents[img_idx]\n",
    "\n",
    "        eeg_data = self.eeg_data[idx]\n",
    "\n",
    "        return {\n",
    "            \"img_path\": str(img_path),\n",
    "            \"img_latent\": img_latent,\n",
    "            \"eeg_data\": eeg_data,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0e9017",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import cast\n",
    "from torch.utils.data import random_split\n",
    "from torch import manual_seed\n",
    "\n",
    "config = EEGDatasetConfig()\n",
    "\n",
    "\n",
    "def make_datasets(\n",
    "    config: EEGDatasetConfig,\n",
    "    model: str = \"synclr\",\n",
    "    seed: int = 42,\n",
    "    train_val_split: float = 0.8,\n",
    ") -> tuple[EEGDataset, EEGDataset, EEGDataset]:\n",
    "    train_dataset = EEGDataset(config, split=\"train\", model_name=model)\n",
    "    test_dataset = EEGDataset(config, split=\"test\", model_name=model)\n",
    "\n",
    "    split_rng = manual_seed(seed)\n",
    "    train_dataset, val_dataset = random_split(\n",
    "        train_dataset, [train_val_split, 1 - train_val_split], generator=split_rng\n",
    "    )\n",
    "    train_dataset = cast(EEGDataset, train_dataset)\n",
    "    val_dataset = cast(EEGDataset, val_dataset)\n",
    "    test_dataset = cast(EEGDataset, test_dataset)\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "\n",
    "#train_dataset, val_dataset, test_dataset = make_datasets(\n",
    "#    config, model=\"synclr\", seed=42\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5d67e1",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca985fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://github.com/eeyhsong/NICE-EEG\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import einops\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "\n",
    "class EEGEncoderConfig(BaseModel):\n",
    "    embed_dim: int = 40\n",
    "    encoded_dim: int = 1440  # Result of embed dim * final spatial * final temporal\n",
    "    proj_dim: int = 768\n",
    "\n",
    "    temporal_kernel_size: int = 25\n",
    "    spatial_kernel_size: int = 17\n",
    "    temporal_pool_size: int = 41\n",
    "    temporal_stride: int = 1\n",
    "    hidden_dim: int = 40\n",
    "    dropout: float = 0.5\n",
    "\n",
    "\n",
    "class DebugLayer(nn.Module):\n",
    "    def __init__(self, note: str = \"\"):\n",
    "        super(DebugLayer, self).__init__()\n",
    "        self.note = note\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f\"(debug): {x.shape} - {self.note}\")\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim,\n",
    "        temporal_kernel_size,\n",
    "        spatial_kernel_size,\n",
    "        temporal_pool_size,\n",
    "        temporal_stride,\n",
    "        hidden_dim,\n",
    "        dropout,\n",
    "    ):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "\n",
    "        self.spatiotemporal_conv = nn.Sequential(\n",
    "            nn.Conv2d(1, hidden_dim, kernel_size=(1, temporal_kernel_size)),\n",
    "            nn.AvgPool2d(\n",
    "                kernel_size=(1, temporal_pool_size), stride=(1, temporal_stride)\n",
    "            ),\n",
    "            nn.BatchNorm2d(hidden_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(hidden_dim, hidden_dim, kernel_size=(spatial_kernel_size, 1)),\n",
    "            nn.BatchNorm2d(hidden_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Conv2d(hidden_dim, embed_dim, kernel_size=(1, 1)),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = einops.rearrange(x, \"b s t -> b 1 s t\")\n",
    "        x = self.spatiotemporal_conv(x)\n",
    "        x = self.projection(x)\n",
    "        x = einops.rearrange(x, \"b e (s) (t) -> b (s t) e\")\n",
    "        return x\n",
    "\n",
    "\n",
    "class EEGEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: EEGEncoderConfig = EEGEncoderConfig(),\n",
    "    ):\n",
    "        super(EEGEncoder, self).__init__()\n",
    "\n",
    "        self.patch_embedding = PatchEmbedding(\n",
    "            embed_dim=config.embed_dim,\n",
    "            temporal_kernel_size=config.temporal_kernel_size,\n",
    "            spatial_kernel_size=config.spatial_kernel_size,\n",
    "            temporal_pool_size=config.temporal_pool_size,\n",
    "            temporal_stride=config.temporal_stride,\n",
    "            hidden_dim=config.hidden_dim,\n",
    "            dropout=config.dropout,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.patch_embedding(x)\n",
    "        x = x.flatten(start_dim=1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class LatentProjector(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int = 1440,\n",
    "        proj_dim: int = 768,\n",
    "        dropout: float = 0.5,\n",
    "    ):\n",
    "        super(LatentProjector, self).__init__()\n",
    "\n",
    "        self.l_proj = nn.Linear(embed_dim, proj_dim)\n",
    "        self.l_inner = nn.Sequential(\n",
    "            nn.GELU(),\n",
    "            nn.Linear(proj_dim, proj_dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(proj_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x_res = x = self.l_proj(x)\n",
    "        x = self.l_inner(x) + x_res\n",
    "        x = self.norm(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "eeg_config = EEGEncoderConfig()\n",
    "eeg_encoder = EEGEncoder(eeg_config).to(device)\n",
    "\n",
    "#ex_data = train_dataset[0][\"eeg_data\"].unsqueeze(0).to(device).to(torch.float32)\n",
    "#eeg_encoder(ex_data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc5da92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from lightning import LightningModule\n",
    "\n",
    "\n",
    "class NICEConfig(BaseModel):\n",
    "    project_dim: int = 768\n",
    "    eeg_latent_dim: int = 1440\n",
    "    img_latent_dim: int = 768\n",
    "\n",
    "    batch_size: int = 1024\n",
    "    eval_batch_size: int = 200\n",
    "    learning_rate: float = 3e-4\n",
    "    beta1: float = 0.9\n",
    "    beta2: float = 0.999\n",
    "    num_workers: int = 4\n",
    "\n",
    "\n",
    "class NICEModel(LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        eeg_config: EEGEncoderConfig | None = EEGEncoderConfig(),\n",
    "        eeg_projector: LatentProjector | None = None,\n",
    "        img_projector: LatentProjector | None = None,\n",
    "        config: NICEConfig = NICEConfig(),\n",
    "        dataset_config: EEGDatasetConfig = EEGDatasetConfig(),\n",
    "        model_name: str = \"synclr\",\n",
    "        data_seed: int = 42,\n",
    "        eeg_encoder: EEGEncoder | None = None,\n",
    "    ):\n",
    "        super(NICEModel, self).__init__()\n",
    "\n",
    "        assert (\n",
    "            eeg_config or eeg_encoder\n",
    "        ), \"Either eeg_config or eeg_encoder must be provided.\"\n",
    "\n",
    "        self.config = config\n",
    "        self.eeg_encoder = eeg_encoder or EEGEncoder(eeg_config)  # type: ignore #\n",
    "        self.eeg_projector = eeg_projector or LatentProjector(\n",
    "            embed_dim=config.eeg_latent_dim,\n",
    "            proj_dim=config.project_dim,\n",
    "        )\n",
    "        self.img_projector = img_projector or LatentProjector(\n",
    "            embed_dim=config.img_latent_dim,\n",
    "            proj_dim=config.project_dim,\n",
    "        )\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.train_dataset, self.val_dataset, self.test_dataset = make_datasets(\n",
    "            dataset_config,\n",
    "            model=model_name,\n",
    "            seed=data_seed,\n",
    "        )\n",
    "        self.model_name = model_name\n",
    "\n",
    "        self.save_hyperparameters(\n",
    "            \"eeg_encoder\",\n",
    "            \"eeg_projector\",\n",
    "            \"img_projector\",\n",
    "            \"config\",\n",
    "            \"dataset_config\",\n",
    "            \"model_name\",\n",
    "            \"data_seed\",\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        \"\"\"Return the training dataloader.\"\"\"\n",
    "        return (\n",
    "            torch.utils.data.DataLoader(\n",
    "                self.train_dataset,\n",
    "                batch_size=self.config.batch_size,\n",
    "                shuffle=True,\n",
    "                num_workers=self.config.num_workers,\n",
    "                pin_memory=True,\n",
    "            )\n",
    "            if self.train_dataset\n",
    "            else None\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        \"\"\"Return the validation dataloader.\"\"\"\n",
    "        return (\n",
    "            torch.utils.data.DataLoader(\n",
    "                self.val_dataset,\n",
    "                batch_size=self.config.eval_batch_size,\n",
    "                shuffle=False,\n",
    "                num_workers=self.config.num_workers,\n",
    "                pin_memory=True,\n",
    "            )\n",
    "            if self.val_dataset\n",
    "            else None\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        \"\"\"Return the test dataloader.\"\"\"\n",
    "        return (\n",
    "            torch.utils.data.DataLoader(\n",
    "                self.test_dataset,\n",
    "                batch_size=self.config.eval_batch_size,\n",
    "                shuffle=False,\n",
    "                num_workers=self.config.num_workers,\n",
    "                pin_memory=True,\n",
    "            )\n",
    "            if self.test_dataset\n",
    "            else None\n",
    "        )\n",
    "\n",
    "    def forward(self, img_latent: torch.Tensor, eeg_data: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass through the model.\"\"\"\n",
    "        # Encode EEG data\n",
    "        eeg_latent = self.eeg_encoder(eeg_data)\n",
    "        eeg_latent = self.eeg_projector(eeg_latent)\n",
    "\n",
    "        # Project image latent\n",
    "        img_latent = self.img_projector(img_latent)\n",
    "\n",
    "        # Compute similarity\n",
    "        sim = eeg_latent @ img_latent.T \n",
    "\n",
    "        return sim\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Configure optimizers for the model.\"\"\"\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.parameters(),\n",
    "            lr=self.config.learning_rate,\n",
    "            betas=(self.config.beta1, self.config.beta2),\n",
    "        )\n",
    "        return optimizer\n",
    "\n",
    "    def get_loss(self, sim: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Compute cross-entropy loss.\"\"\"\n",
    "        labels = torch.arange(sim.size(0), device=sim.device)\n",
    "        loss_e = self.loss(sim, labels)\n",
    "        loss_i = self.loss(sim.T, labels)\n",
    "        loss = (loss_e + loss_i) / 2\n",
    "        return loss\n",
    "\n",
    "    def get_top_n_accuracy(self, sim: torch.Tensor, n: int = 1) -> float:\n",
    "        \"\"\"Compute top-n accuracy.\"\"\"\n",
    "        labels = torch.arange(sim.size(0), device=sim.device)\n",
    "        top_n = sim.topk(n, dim=-1).indices\n",
    "        correct = (top_n == labels.unsqueeze(1)).float()\n",
    "        return (correct.sum() / correct.numel()).item()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"Training step for the model.\"\"\"\n",
    "        img_latent = batch[\"img_latent\"]\n",
    "        eeg_data = batch[\"eeg_data\"]\n",
    "\n",
    "        sim = self(img_latent, eeg_data)\n",
    "        loss = self.get_loss(sim)\n",
    "\n",
    "        self.log(\"train/loss\", loss, prog_bar=True, on_step=True, on_epoch=False)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        img_latent = batch[\"img_latent\"]\n",
    "        eeg_data = batch[\"eeg_data\"]\n",
    "\n",
    "        sim = self(img_latent, eeg_data)\n",
    "        loss = self.get_loss(sim)\n",
    "\n",
    "        top1_acc = self.get_top_n_accuracy(sim, n=1)\n",
    "        top3_acc = self.get_top_n_accuracy(sim, n=3)\n",
    "        top5_acc = self.get_top_n_accuracy(sim, n=5)\n",
    "\n",
    "        self.log(\"val/loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.log(\"val/top1_acc\", top1_acc, prog_bar=False, on_step=False, on_epoch=True)\n",
    "        self.log(\"val/top3_acc\", top3_acc, prog_bar=False, on_step=False, on_epoch=True)\n",
    "        self.log(\"val/top5_acc\", top5_acc, prog_bar=False, on_step=False, on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        img_latent = batch[\"img_latent\"]\n",
    "        eeg_data = batch[\"eeg_data\"]\n",
    "\n",
    "        sim = self(img_latent, eeg_data)\n",
    "        loss = self.get_loss(sim)\n",
    "        top1_acc = self.get_top_n_accuracy(sim, n=1)\n",
    "        top3_acc = self.get_top_n_accuracy(sim, n=3)\n",
    "        top5_acc = self.get_top_n_accuracy(sim, n=5)\n",
    "\n",
    "        self.log(\"test/loss\", loss, prog_bar=True, on_step=False, on_epoch=False)\n",
    "        self.log(\"test/top1_acc\", top1_acc, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.log(\n",
    "            \"test/top3_acc\", top3_acc, prog_bar=False, on_step=False, on_epoch=True\n",
    "        )\n",
    "        self.log(\n",
    "            \"test/top5_acc\", top5_acc, prog_bar=False, on_step=False, on_epoch=True\n",
    "        )\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e13135",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff06a23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch import Trainer\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "\n",
    "def setup_and_train(\n",
    "    config: NICEConfig = NICEConfig(),\n",
    "    dataset_config: EEGDatasetConfig = EEGDatasetConfig(),\n",
    "    model_name: str = \"synclr\",\n",
    "    data_seed: int = 42,\n",
    "    epochs: int = 100,\n",
    "    save_top_k: int = 1,\n",
    "):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = NICEModel(\n",
    "        config=config,\n",
    "        dataset_config=dataset_config,\n",
    "        model_name=model_name,\n",
    "        data_seed=data_seed,\n",
    "    ).to(device)\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor=\"val/loss\",\n",
    "        filename=\"checkpoint/{epoch:02d}-{val/loss:.2f}\",\n",
    "        save_top_k=save_top_k,\n",
    "        mode=\"min\",\n",
    "    )\n",
    "\n",
    "    logger = TensorBoardLogger(\n",
    "        save_dir=\"logs\",\n",
    "        name=model_name,\n",
    "        default_hp_metric=False,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        max_epochs=epochs,\n",
    "        callbacks=[checkpoint_callback],\n",
    "        logger=logger,\n",
    "        enable_progress_bar=True,\n",
    "        accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "    )\n",
    "\n",
    "    trainer.fit(model)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b443d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SYNCLR = True\n",
    "\n",
    "if TRAIN_SYNCLR:\n",
    "    setup_and_train(\n",
    "        model_name=\"synclr\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb23a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_ALIGNED_SYNCLR = True\n",
    "\n",
    "if TRAIN_ALIGNED_SYNCLR:\n",
    "    setup_and_train(\n",
    "        model_name=\"aligned_synclr\",\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
